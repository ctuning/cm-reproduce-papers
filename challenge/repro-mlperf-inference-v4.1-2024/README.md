Anyone is welcome to participate in the MLPerf inference v4.1 community submission to benchmark any commodity hardware and software using MLPerf and CM.

Pease follow this [official MLCommons documentation](https://docs.mlcommons.org/inference) to run the MLPerf inference v4.1 benchmark
and submit results. 

We recommend to use [this guide](https://access.cknowledge.org/playground/?action=install) 
to install CM instead of the MLCommons one since the last one is not yet stable.
You can read more about the CM automation for MLPerf in this [white paper](https://arxiv.org/abs/2406.16791).

If you encounter issues with the CM automation for MLPerf, please report them [here](https://github.com/mlcommons/cm4mlops/issues).

Remember that you need to be an official MLCommons member to submit your results [here](https://submissions-ui.mlcommons.org).

If you have questions about MLPerf inference benchmarks, please contact David Kanter, Scott Wasson or Pablo Gonzalez Mesa.

**Deadline: 26 July 2024**

*We thank [cKnowledge.org](https://cKnowledge.org),
[cTuning.org](https://cTuning.org) and 
[MLCommons](https://mlcommons.org) for sponsoring 
the development of the [open-source CM4MLops and CM4MLPerf automation](https://github.com/mlcommons/cm4mlops).*
